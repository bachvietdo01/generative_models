{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import ssl \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: construct target data and initial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian import Sampleable\n",
    "\n",
    "class MNISTSampler(nn.Module, Sampleable):\n",
    "    \"\"\"\n",
    "    Sampleable wrapper for the MNIST dataset\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dataset = datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ])\n",
    "        )\n",
    "        self.dummy = nn.Buffer(torch.zeros(1)) # Will automatically be moved when self.to(...) is called...\n",
    "        self.dim = list(self.dataset[0][0].shape)\n",
    "\n",
    "    def sample(self, num_samples: int) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - num_samples: the desired number of samples\n",
    "        Returns:\n",
    "            - samples: shape (batch_size, c, h, w)\n",
    "            - labels: shape (batch_size, label_dim)\n",
    "        \"\"\"\n",
    "        if num_samples > len(self.dataset):\n",
    "            raise ValueError(f\"num_samples exceeds dataset size: {len(self.dataset)}\")\n",
    "\n",
    "        indices = torch.randperm(len(self.dataset))[:num_samples]\n",
    "        samples, labels = zip(*[self.dataset[i] for i in indices])\n",
    "        samples = torch.stack(samples).to(self.dummy)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64).to(self.dummy.device)\n",
    "        return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = MNISTSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build Gaussian Conditional Probability Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian import Sampleable\n",
    "\n",
    "class StandardNormal(nn.Module, Sampleable):\n",
    "    \"\"\"\n",
    "    Sampleable wrapper around torch.randn\n",
    "    \"\"\"\n",
    "    def __init__(self, shape: List[int], std: float = 1.0):\n",
    "        \"\"\"\n",
    "        shape: shape of sampled data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.dummy = nn.Buffer(torch.zeros(1)) # Will automatically be moved when self.to(...) is called...\n",
    "\n",
    "    def sample(self, num_samples) -> torch.Tensor:\n",
    "        return self.std * torch.randn(num_samples, *self.shape).to(self.dummy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAlpha:\n",
    "    \"\"\"\n",
    "    Implements alpha_t = t\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Check alpha_t(0) = 0\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1,1,1)), torch.zeros(1,1,1,1)\n",
    "        )\n",
    "        # Check alpha_1 = 1\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1,1,1)), torch.ones(1,1,1,1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - alpha_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return t\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates d/dt alpha_t.\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - d/dt alpha_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return torch.ones_like(t)\n",
    "        \n",
    "\n",
    "class LinearBeta:\n",
    "    \"\"\"\n",
    "    Implements beta_t = 1-t\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Check beta_0 = 1\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1,1,1)), torch.ones(1,1,1,1)\n",
    "        )\n",
    "        # Check beta_1 = 0\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1,1,1)), torch.zeros(1,1,1,1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - beta_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return 1-t\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates d/dt alpha_t.\n",
    "        Args:\n",
    "            - t: time (num_samples, 1, 1, 1)\n",
    "        Returns:\n",
    "            - d/dt alpha_t (num_samples, 1, 1, 1)\n",
    "        \"\"\"\n",
    "        return - torch.ones_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConditionalProbabilityPath(nn.Module):\n",
    "    def __init__(self, p_data: Sampleable, alpha: LinearAlpha, beta: LinearBeta):\n",
    "        super().__init__()\n",
    "        p_init = StandardNormal(shape = p_data.dim, std = 1.0)\n",
    "        self.p_init = p_init\n",
    "        self.p_data = p_data\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_marginal_path(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        num_samples = t.shape[0]\n",
    "        # Sample conditioning variable z ~ p(z)\n",
    "        z, _ = self.sample_conditioning_variable(num_samples) # (num_samples, c, h, w)\n",
    "        # Sample conditional probability path x ~ p_t(x|z)\n",
    "        x = self.sample_conditional_path(z, t) # (num_samples, c, h, w)\n",
    "        return x\n",
    "\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor:\n",
    "        return self.p_data.sample(num_samples)\n",
    "\n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:        \n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        alpha_t = self.alpha(t) # (num_samples, 1, 1, 1)\n",
    "        beta_t = self.beta(t) # (num_samples, 1, 1, 1)\n",
    "        dt_alpha_t = self.alpha.dt(t) # (num_samples, 1, 1, 1)\n",
    "        dt_beta_t = self.beta.dt(t) # (num_samples, 1, 1, 1)\n",
    "\n",
    "        return (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x\n",
    "\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        alpha_t = self.alpha(t)\n",
    "        beta_t = self.beta(t)\n",
    "        return (z * alpha_t - x) / beta_t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct conditional probability path\n",
    "path = GaussianConditionalProbabilityPath(\n",
    "    p_data = p_data,\n",
    "    alpha = LinearAlpha(),\n",
    "    beta = LinearBeta()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.5: Visualize the conditional path from $X_0$ to a data point in $p_{\\text{data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultility import plot_conditional_path\n",
    "\n",
    "plot_conditional_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Learn vector the field $u(x, t)$ with MLP neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from unet import MNISTUNet\n",
    "\n",
    "class ConditionalFlowMatchingTrainer(Trainer):\n",
    "    def __init__(self, path: GaussianConditionalProbabilityPath, model: MNISTUNet, **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.path = path\n",
    "\n",
    "    def get_train_loss(self, batch_size: int) -> torch.Tensor:\n",
    "      z, _ = self.path.p_data.sample(batch_size)\n",
    "      t = torch.rand(batch_size, 1, 1, 1).to(z.device)\n",
    "      x = self.path.sample_conditional_path(z, t)\n",
    "      u_theta = self.model(x, t)\n",
    "      u_ref = self.path.conditional_vector_field(x, z, t)\n",
    "\n",
    "      return torch.mean((u_theta - u_ref)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct learnable vector field\n",
    "flow_model = MNISTUNet(\n",
    "    channels = [32, 64, 128],\n",
    "    num_residual_layers = 4,\n",
    "    t_embed_dim = 128,\n",
    ")\n",
    "\n",
    "\n",
    "PRETRAINED_PATH = 'trained/mnist_unet_fm_10000.pt'\n",
    "\n",
    "if os.path.exists(PRETRAINED_PATH):\n",
    "    flow_model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=torch.device('cpu')))\n",
    "else:\n",
    "    # Construct trainer\n",
    "    trainer = ConditionalFlowMatchingTrainer(path, flow_model)\n",
    "    losses = trainer.train(num_epochs=5000, device=device, lr=1e-3, batch_size=250)\n",
    "    torch.save(flow_model.state_dict(), \"trained/mnist_unet_fm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Generate samples from learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ode import LearnedVectorFieldODE, EulerSimulator\n",
    "from ultility import plot_generated_sample\n",
    "\n",
    "\n",
    "ode = LearnedVectorFieldODE(flow_model)\n",
    "simulator = EulerSimulator(ode)\n",
    "\n",
    "plot_generated_sample(path, simulator, num_timesteps = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
