{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this gentle introduction, we are going to construct a Gaussian probability path defined by $p_t(\\cdot) = N(\\cdot | \\alpha_t z, \\beta_t \\cdot I)$, where $\\alpha_t = t$, $\\beta_t = \\sqrt{1 - t}$, and $t$ is a time index in the interval $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: construct target data and initial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian import Gaussian, GaussianMixture\n",
    "from ultility import plot_comparison_heatmap\n",
    "\n",
    "# Constants for the duration of our use of Gaussian conditional probability paths, to avoid polluting the namespace...\n",
    "PARAMS = {\n",
    "    \"scale\": 15.0,\n",
    "    \"target_scale\": 10.0,\n",
    "    \"target_std\": 1.0,\n",
    "}\n",
    "\n",
    "p_init = Gaussian.standard(dim=2, std = 1.0).to(device)\n",
    "p_data = GaussianMixture.symmetric_2D(nmodes=11, std=PARAMS[\"target_std\"], scale=PARAMS[\"target_scale\"]).to(device)\n",
    "plot_comparison_heatmap(p_init, p_data, PARAMS['scale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build Gaussian Conditional Probability Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian import Sampleable\n",
    "\n",
    "class StandardNormal(nn.Module, Sampleable):\n",
    "    \"\"\"\n",
    "    Sampleable wrapper around torch.randn\n",
    "    \"\"\"\n",
    "    def __init__(self, shape: List[int], std: float = 1.0):\n",
    "        \"\"\"\n",
    "        shape: shape of sampled data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.dummy = nn.Buffer(torch.zeros(1)) # Will automatically be moved when self.to(...) is called...\n",
    "\n",
    "    def sample(self, num_samples) -> torch.Tensor:\n",
    "        return self.std * torch.randn(num_samples, *self.shape).to(self.dummy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAlpha:\n",
    "    \"\"\"\n",
    "    Implements alpha_t = t\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Check alpha_t(0) = 0\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1,1,1)), torch.zeros(1,1,1,1)\n",
    "        )\n",
    "        # Check alpha_1 = 1\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1,1,1)), torch.ones(1,1,1,1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - alpha_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return t\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates d/dt alpha_t.\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - d/dt alpha_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return torch.ones_like(t)\n",
    "        \n",
    "\n",
    "class SquareRootBeta:\n",
    "    \"\"\"\n",
    "    Implements beta_t = rt(1-t)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Check beta_0 = 1\n",
    "        assert torch.allclose(\n",
    "            self(torch.zeros(1,1,1,1)), torch.ones(1,1,1,1)\n",
    "        )\n",
    "        # Check beta_1 = 0\n",
    "        assert torch.allclose(\n",
    "            self(torch.ones(1,1,1,1)), torch.zeros(1,1,1,1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - beta_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return torch.sqrt(1 - t)\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates d/dt alpha_t.\n",
    "        Args:\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - d/dt alpha_t (num_samples, 1)\n",
    "        \"\"\"\n",
    "        return - 0.5 / (torch.sqrt(1 - t) + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConditionalProbabilityPath(nn.Module):\n",
    "    def __init__(self, p_data: Sampleable, alpha: LinearAlpha, beta: SquareRootBeta):\n",
    "        super().__init__()\n",
    "        p_init = StandardNormal(shape = [p_data.dim], std = 1.0)\n",
    "        self.p_init = p_init\n",
    "        self.p_data = p_data\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_marginal_path(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        num_samples = t.shape[0]\n",
    "        # Sample conditioning variable z ~ p(z)\n",
    "        z, _ = self.sample_conditioning_variable(num_samples) # (num_samples, c, h, w)\n",
    "        # Sample conditional probability path x ~ p_t(x|z)\n",
    "        x = self.sample_conditional_path(z, t) # (num_samples, c, h, w)\n",
    "        return x\n",
    "\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor:\n",
    "        return self.p_data.sample(num_samples)\n",
    "\n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        alpha_t = self.alpha(t) # (num_samples, 1, 1, 1)\n",
    "        beta_t = self.beta(t) # (num_samples, 1, 1, 1)\n",
    "        dt_alpha_t = self.alpha.dt(t) # (num_samples, 1, 1, 1)\n",
    "        dt_beta_t = self.beta.dt(t) # (num_samples, 1, 1, 1)\n",
    "\n",
    "        return (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x\n",
    "\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        alpha_t = self.alpha(t)\n",
    "        beta_t = self.beta(t)\n",
    "        return (z * alpha_t - x) / beta_t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct conditional probability path\n",
    "path = GaussianConditionalProbabilityPath(\n",
    "    p_data = p_data,\n",
    "    alpha = LinearAlpha(),\n",
    "    beta = SquareRootBeta()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.5: Visualize the conditional path from $X_0$ to a data point in $p_{\\text{data}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultility import plot_conditional_path\n",
    "\n",
    "plot_conditional_path(path , p_init, p_data, PARAMS['scale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Learn vector the field $u(x, t)$ with MLP neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPVectorField(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-parameterization of the learned vector field u_t^theta(x)\n",
    "    \"\"\"\n",
    "    def get_mlp(self, dims: List[int], activation: Type[torch.nn.Module] = torch.nn.SiLU):\n",
    "        mlp = []\n",
    "        for idx in range(len(dims) - 1):\n",
    "            mlp.append(torch.nn.Linear(dims[idx], dims[idx + 1]))\n",
    "            if idx < len(dims) - 2:\n",
    "                mlp.append(activation())\n",
    "        return torch.nn.Sequential(*mlp)\n",
    "\n",
    "    def __init__(self, dim: int, hiddens: List[int]):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.net = self.get_mlp([dim + 1] + hiddens + [dim])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, dim)\n",
    "        Returns:\n",
    "        - u_t^theta(x): (bs, dim)\n",
    "        \"\"\"\n",
    "        xt = torch.cat([x,t], dim=-1)\n",
    "        return self.net(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "class ConditionalFlowMatchingTrainer(Trainer):\n",
    "    def __init__(self, path: GaussianConditionalProbabilityPath, model: MLPVectorField, **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.path = path\n",
    "\n",
    "    def get_train_loss(self, batch_size: int) -> torch.Tensor:\n",
    "      z = self.path.p_data.sample(batch_size)\n",
    "      t = torch.rand(batch_size, 1)\n",
    "      x = self.path.sample_conditional_path(z, t)\n",
    "      u_theta = self.model(x, t)\n",
    "      u_ref = self.path.conditional_vector_field(x, z, t)\n",
    "\n",
    "      return torch.mean((u_theta - u_ref)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct learnable vector field\n",
    "flow_model = MLPVectorField(dim=2, hiddens=[1024,16])\n",
    "\n",
    "# Construct trainer\n",
    "trainer = ConditionalFlowMatchingTrainer(path, flow_model)\n",
    "losses = trainer.train(num_epochs=5000, device=device, lr=1e-3, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(flow_model.state_dict(), \"trained/gmm_mlp_fm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Generate samples from learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ode import LearnedVectorFieldODE, EulerSimulator\n",
    "\n",
    "num_samples = 1000\n",
    "num_timesteps = 300\n",
    "num_marginals = 3\n",
    "\n",
    "ode = LearnedVectorFieldODE(flow_model)\n",
    "simulator = EulerSimulator(ode)\n",
    "x0 = path.p_init.sample(num_samples) # (num_samples, 2)\n",
    "ts = torch.linspace(0.0, 1.0, num_timesteps).view(1,-1,1).expand(num_samples,-1,1).to(device) # (num_samples, nts, 1)\n",
    "xts = simulator.simulate_with_trajectory(x0, ts) # (bs, nts, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from ultility import plot_generated_sample\n",
    "\n",
    "plot_generated_sample(xts, ts, p_init, p_data, scale = PARAMS['scale'], num_samples=num_samples, num_timesteps=num_timesteps, num_marginals=num_marginals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
